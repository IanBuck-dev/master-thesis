\chapter{Conclusion}
\label{chap:Conclusion}

In this work, the feasibility of two mayor use cases for \gls{ml}-based interpolation, e.g., interpolation for a single location and areal interpolation, was explored given the context of urban local \gls{ta} mapping and an upper bound for \gls{rmse} values for both use cases were established, comparing different \gls{ml} models against each other and testing their suitability to model the urban microclimate.\\
For interpolation of a single station, sensor readings from Netatmo and Sensor.Community were collected for the month of June 2023 from the areas of Hamburg and Stuttgart in Germany. The lowest \gls{rmse} was achieved by \gls{hgb} of around 0.5, depending on the train/test split, showing the ability of the \gls{ml} model to accurately predict \gls{ta} for a specific location based on surrounding stations, even when stations have missing data and are not available at all times. \gls{hgb} was further evaluated for the region of Hamburg with Netatmo data to inspect feature (permutation) importance, distances between neighbours, and data availability of neighbours. A look at the permutation importance suggests, that only a small percentage of neighbouring stations, i.e., those that have a very similar temperature curve, have a huge influence on the prediction quality, while other neighbours and other features have little to no influence, suggesting that for single station interpolation one might only want to focus on collecting temperature data and ignore other types of readings, reducing data pre-processing overhead. The distance between neighbours had only a minimal influence up to 7km, suggesting that neighbouring stations should not be located further than 10km away. Experiments with sampling the available data per station to simulate non-stationary sensors showed that even with a big number of moving sensors, as long as there are few stationary sensors, prediction quality is always high; however, when those stationary sensors get removed and all sensors are moving, errors only stayed low when many readings, e.g., 1000 per station, were available, which could be achieved for example by a bus that visits a location more than once every hour ($30 days * 24h = 720$). \\
Areal interpolation on the other hand was evaluated with Sensor.Community data from Stuttgart and performed slightly worse than single station interpolation at a \gls{rmse} of 1.4-1.5 for a single timestamp on the 19.06.2023 14h, an especially hot day in Stuttgart, during the day but with a significantly lower \gls{r2} score of a maximum of 0.2 with some models. In comparison, on the same day but in the night at 5AM, the \gls{rmse} was significantly lower down to 0.65-0.7 for \gls{hgb} with \gls{r2} scores around 0.7, suggesting that during the day other factors influence the urban climate that we do not capture, such as \gls{lst} or solar radiance, which should be explored in the future. Compared to \gls{ok}, a geostatistical model, the best predictor, i.e., \gls{hgb}, had similar \gls{rmse} and \gls{r2} scores when tested against both timestamps; however, the \gls{rmse} and \gls{r2} values highly depended on the way the data was split, as changing the random seed for the train/test splitting revealed, suggesting as there is information missing for the model to accurately evaluate specific stations and sensor readings. One such missing factor could be the height of sensors, as a comparison with a DWD reference station revealed, that the temperature at 2m and 5cm differs by a lot, which means that a hotter station that is closer to the ground as a cooler station nearby could be correct; however, in the model this information is not captured, resulting in training data were two totally different sensor readings are located next to each other.\\
Finally, we have shown that \gls{hgb} seems to be a very flexible model that outperforms similar \gls{rf} models, given that there is slightly more data to train on, and also has the added bonus of natively handling missing values, and is suitable for \gls{ta} interpolation in the urban setting. Due to the limited scope, there were many aspects which could be further improved such as using 10-fold cross validation everywhere and running exhaustive grid searches to find the most optimal model setups, collecting more data for more cities, eventually with better ways to validate our findings, i.e., through more densely located reference stations, improving the \gls{qc} process, using more precise remote sensing data, and much more.

\section{Future Outlook}

There are many different directions in which this work could be extended. The areal interpolation use-case offers much room for improvement by using more precise remote sensing measurements or incorporating the temporal aspect, as well as the single station interpolation use-case, which could be turned into an actual study to collect data and evaluate how this data could improve the monitoring of the urban climate.\\
Additionally, we didn't compare \gls{ml} models to Neural Networks, which could be even more powerful in their predictive capabilities. Another starting point could also be data pre-processing and \gls{qc} or the influence of sensor height (and location) on the prediction quality. Lastly, working with spatial data purely in Python was quite slow, therefore it could make sense to improve tooling around the interpolation and \gls{pws} data collection to provide researchers with more flexible ways of conducting research and reducing barriers of getting started.