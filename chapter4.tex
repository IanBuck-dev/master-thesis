\chapter{Machine Learning Model Design}
\label{chap:Machine Learning Model Design}
% 10 pages?

In recent times, the area of machine learning has seen big advancements in terms of model size and complexity. Especially in the area of generative AI, e.g. text and image generation, models such as OpenAI's ChatGPT~\cite{openai2023gpt4} have generated significant hype for the possiblity of use of AI. As a result, the question arises what benefits AI can bring to other areas of application, such as sensing applications in the context of smart cities. In this chapter, we will discuss the usage of AI in the context of smart city, more precisely in the context of data enrichment via interpolation, and outline possible advantages and disadvantages compared to traditional (geo-) statistical methods.\\
First of all, we need to clarify what is meant by artificial intelligence (AI), machine learning (ML) and deep learning (DL). AI is a broad term that is used to describe the ability to perform tasks, that are usually associated with human intelligence. ML is a subfield of AI, that focuses on the ability of a system to learn from data without being explicitly programmed to do so. Finally, DL is a subfield of ML, that uses artificial neural networks, which imitate the structure of the human brain, to learn from data and perform various tasks.\\
In this work, we will focus primarely on DL models, as these offer the possibility to learn complex patterns and dependencies, which could be beneficial in the complex context of smart city and temperature interpolation. The universal approximation theorem states, that a feedforward network with a single hidden layer containing a finite number of neurons can approximate any continous function~\cite{hornik1989multilayer}. This statement emphasizes the power of DL models. However, it does not state how many neurons are needed to approximate the function and gives no indication on how to train the network. In practice there is lot of trial and error involved when designing DL models.\\
Additionally, we will also take a brief look on traditional ML models, such as linear regression models, as these are usually very easy to implement and can be used as another baseline for comparison, which trade speed of setup and execution against complexity and potentially less accuracy.\\
\\
In the following, we will first discuss fundamental model assumptions given by the data layer. Afterwards, we will discuss the potential design of a ML model for temperature interpolation, including the architecture as well as potential input features and possible pre-processing steps. The model is then implemented and later evaluated in chapter \ref{chap:Evaluation}.

% Goal of this chapter

\section{Model Assumptions from the Data Layer}

The data-layer exposes a variety of single data-points for various features for different locations for current and past points in time. As meteorological research and analysis activities are usually in need of gridded or continuous data \cite{sekulic2020spatio}, the goal of this chapter is to design a model that is capable of turning these single data-points into a gridded data layer of air temperature where each cell has a specific air temperature value. Features other than air temperature could further improve the predecition quality, like how \cite{alonso2020new} suggests that in their study the Normalized Difference Vegetation Index (NDVI) and Modified Normalized Difference Water Index (MNDWI) have a strong impact on their estimation model. Therefore, we discuss how additional features can be included in the model.\\
This model should then be deloyed inside the \textit{service-layer} and act as a building block for further temperature related research and analysis, as air temperature is an important variable for research in agronomy, meteorology, hydrology, ecology and many other fields of application and could be used for UHI detection in the context of smart cities. The general idea and architecture behind the model should not only be applicable for air temperature, but also other types of output features.

% TODO: summary of the assumptions for the geolocation data

\subsection{Incorporating Geolocation Data}

As discussed in chapter \ref{chap:Related Work}, there is a dependency between air temperature and other meteological features and the location of the sensors. The closer a sensor is to another sensor, the more correlated the sensor readings should be.
In geo-statistics, this is called spatial autocorrelation and can be defined traditionally with the Moran's I index~\cite{moran1948interpretation} or the Geary's coefficient~\cite{geary1954contiguity}.
% TODO: explain autocorrelation and the implications for ML models
% model assumptions: no ML algorithm that expects independence between observations
% feature selection: including all correlated features can lead to overfitting
% spatial feature engineering: add additional features that encode the spatial relationship between sensors
% spatial smoothing:
% spatial aware algorithms: geographically weighted regression (GWR) 

This relationship is however greatly influenced by the type of feature and the location of a sensor. As explained in \cite{oke2006guideline}, the sensor placement in the urban environment is a complex task, as the urban environment is highly dynamic and sensors can be influenced by highly local phenomena, such as air temperarture by heat vents or solar radiation by buildings and surface materials. For this model, we assume that the sensors are placed in a way that the sensor readings are representative for the area they are placed in, even tho in practice this assumption might not hold up, especially for sensors placed by non-experts.\\
Each sensor reading from the data-layer has a location associated with it in the form of longitute, latitude and altitude (if available). This information can be used to calculate the distance between sensors and subsequently the correlation between sensor readings. However, how exactly the distance between two locations is calculated is not trivial. As the earth is a sphere, the distance between two points on the surface is not a straight line. Depending on the application, different distance metrics can be used. For a small area, such as the city of Hamburg, the euclidean distance could be sufficient, as the curvature of the earth for a small distance is negligible. However, for larger areas the geodesic (harvesine) distance might be more appropriate. In this work, we are focussing on a single city including it's surrounding area, therefore the euclidean distance should have a sufficient accuracy while also simplifing the calculation. As the city of Hamburg does not have big differences in elevation, the altitude is not considered in the distance calculation.\\
% grid area -> each data point is in a grid cell. Multiple data points can be in the same grid cell. If this is the case, the readings are averaged. Outliers based on the overall mean temperature are removed. -> this is done in the data layer
The location data can be incorporated into the input data in several ways. The most straight forward way would be to just include the longitude and latitude as input features. Depending on the type of model used, these values might need to be normalised. For example, tree-based models do not require normalisation (cite), as they are not sensitive to the scale of the input features. However, neural networks are very sensitive to the scale of the input features and therefore require normalisation (cite). This approach has the downside, that the distance between sensors is not directly encoded in the input data. If this information is important for the model, it could be included by precalculating the distances between all sensors, however this would increase the complexity of the model especially for large amounts of sensors, as this would result in a quadratic number of input features.\\
The next approach would be to convert the 

- area n x m grid of data points, each grid cell with n x n size (depending on the resulution), 4D space with location as x and y coordinates (euclidean distance -> but only for smaller areas such as city of Hamburg and suroundings), time as z coordinate and feature values as w coordinate.\\

% Timeseries handling





\section{Supervised Learning}
There are multiple learning approaches when it comes to machine learning, especially in the context of smart cities. The main directions are supervised learning, with classification and regression, unsupervised learning with clustering and dimensionaly reduction, and reinforcement learning~\cite{ullah2020applications}. As interpolation of missing data is a form of regression, in this study we use a supervised regression approach to train our ML models.\\
In supervised learning, an AI network is trained with input and target values to create a mapping function. In the context of smart cities, the input values come from the data management layer and consist of historical sensor data, such as humidity, solar radiance, or air temperature, combined with static data as shown in section \ref{subsubsec: integrating static data}. The target variable in this context is air temperature, but the approach can be applied to any other feature, such as humidity or solar radiance.

% TODO: write
\subsection{Dealing with Uncertainty}
- The model has a lot of uncertainty
- model uncertainty in input data? (depending on sensor type, sensor age, placement...)

- dealing with bias
- dealing with variance
-> problem of over-fitting

\subsection{Supervised Regression Models}

\subsubsection{Linear Regression Models}
Most simple type of regression. Expect a linear dependens between dependent and independent variables. Downside, if the dependence is non-linear
- linear regression

\subsubsection{Polynomial Regression Models}
able to fit non-linear dependencies

\subsubsection{Ridge Regression Models}
addresses overfitting to training data

\subsubsection{LASSO Regression}
enforce sparsity in the learned weights

\subsubsection{Bayesian Regression}

\subsubsection{Neural Network Models}
input layer -> hidden layers -> output

The advantages of Multi-layer Perceptron are:
- Capability to learn non-linear models.
- Capability to learn models in real-time (on-line learning) using partial\_fit.

The disadvantages of Multi-layer Perceptron (MLP) include:
- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.
- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.
- MLP is sensitive to feature scaling.

- Loss functions/optimzers: Stochastic Gradient Descend, Adam, L-BFGS


- random forest regression
...

\subsection{Evaluation Metrics}
- Mean absolute error, mean absolute relative, mean quared error, r-squared, root mean squared error (RMSE)

\section{ML Model Deployment}
Deployed as a service. Input ingested -> continous data map as output all 5 min or so (could also be smaller depending on use-case -> trade-off between cost and accurancy)

\subsection{ML Model Retraing}
Need to retrain model from time to time, e.g. if the accurracy drops below a certain threshold.


% Implementation details
- dealing with coordinate systems:
    - projected coordinate system (e.g. UTM) instead of long lat alt values (important for interpolation)

\section{Machine Learning in Geostatistics}

Idea/Hyphothesis: ML outperforms traditional geostatistical models (in certain scenarios) as it is able to capture more complex interdependencies between features and isn't necessarily bound to the (mathematical) assumptions of geostatistical models.

On important point to mention, is that different meteological features have different interpolation techniques, as they beve different (physical) properties. For example, temperature is a scalar value, while wind speed and direction are vector values. Relative humidity on the other hand is a relativ value that is bound between 0 and 1. For percipitation it gets even more complex, as rainfall is highly connected to cloud coverage and movements. In the following, we will take a look at commonly used existing interpolation techniques for different meteological features and discuss data preprocessing steps.

- Dealing with sparse data
- Dealing with extreme weather events (e.g. heat waves, blizzards, etc.)
- how can a ML model be trained if no such data exists for a given area? -> transfer learning

\subsection{Temperature Interpolation}
- difference between air and surface temperature

surface temperature -> solar radiation, surface roughness, emissivity, soil moisture, soil temperature, vegetation cover, snow cover, and surface slope

air temperature -> surface temperature

\subsection{Wind Speed and Direction Interpolation}
prob not easily archivable with ML -> depends on high/low pressure areas

\subsection{Relative Humidity Interpolation}
Data preprocessing: scale to [0, 1]
does not seem to work well with kriging, maybe just nearest neighbor approach/linear, or the input values are just not accurate

\subsection{Percipitation Interpolation}



DeepLearning Lecture
- sequence modeling design criteria
    - handle variable-length sequences
    - track long term dependencies
    - maintain information about order
    - share parameters across the sequence

