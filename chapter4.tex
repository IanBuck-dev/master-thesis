\chapter{Machine Learning-based Interpolation}
\label{chap:Machine Learning based Interpolation}
% 10 pages?

% Intro with small motivation why ML is used
In recent times, the area of machine learning has seen big advancements in terms of model size and complexity. Especially in the area of generative AI, transformer-based neural networks have revolutionised text and image generation. Models such as OpenAI's \textit{ChatGPT}~\cite{openai2023gpt4} or Google's \textit{LaMDA}~\cite{thoppilan2022lamda} have generated significant hype for the possiblity of use of AI\@. Additionally, statements like the universal approximation theorem, which states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continous function~\cite{hornik1989multilayer}, emphasize the potential power of ML models. As a result, the question arises what benefits AI can bring to other areas of application, such as interpolation.\\
In this chapter, we will discuss the usage of ML in the context of data enrichment via interpolation, more precisely in the context of smart cities and urban air temperature interpolation. The ML model will be compared against traditional proven geostatistical model, e.g. Kriging, to outline and discover possible advantages and disadvantages. In general, the idea is to trade the explain- and interpretability of purely statistical-based approaches for model capabilities and accuracy, and the ability to capture more complex (non-linear) dependencies.

\subsubsection{AI vs. Machine Learning vs. Deep Learning}

Before diving deeper into the applications of ML, we need to clarify what is meant by artificial intelligence (AI), machine learning (ML) and deep learning (DL). AI is a broad term that is used to describe the ability to perform tasks, that are usually associated with human intelligence. ML is a subfield of AI, that focuses on the ability of a system to learn from data without being explicitly programmed to do so. Finally, DL is a subfield of ML, that uses artificial neural networks, which imitate the structure of the human brain, to learn from data and perform various tasks.
% TODO: graphic?

\section{Machine-Learning Application Areas in Air Temperarture Interpolation}

As meteorological research and analysis activities are usually in need of gridded or continuous data~\cite{sekulic2020spatio}, interpolation is a really important tool to convert the single data points from ground-based weather stations into continous layers. Interpolation can also be applied to singular sensors in order to fill in missing data that are caused by network outages or to increase the temporal resolution to turn hourly into sub-hourly readings. Especially with the capability to increase the temporal resolution, the question arises how this capability could be combined with moving sensors to increase the spatial coverage of a sensor network. In this work, we will discuss two approaches for the use of interpolation in urban air temperatute sensing:

\begin{itemize}
    \item ML-based interpolation for areas as a substitution for geostatistical methods, e.g. Kriging, to turn individual data points into a continuous grid, possibly with the ability to handle stationary and moving sensor data at the same time
    \item ML-based interpolation to simulate individual virtual sensors that are derived from either low-temporal resolution stationary sensors or moving sensors to increase the temporal resolution of individual sensors and the spatial coverage of sensor networks
\end{itemize}

Research suggests that for fine-granular spatio-temporal urban air temperature maps, a sensor density of at least 1 sensor per km$^{2}$ is needed~\cite{venter2020hyperlocal}, however the denser the sensor network the better the prediction quality, as even inside a single street canyon air temperatures can easily vary by 2 to 3°C~\cite{sugawara2008temperature}.
% TODO: write about other influence sizes, e.g. 10m -> 1km for different features, + the variability of sensors, e.g. inside a street canyon 2-3°c differences.
In order to achive this sensor density as well as to gain insights into previously unobserved areas and to minimise prediction uncertainty for those areas, hybrid approaches combining stationary and moving sensors have shown to work better than purely stationary networks by covering more ground as well as reducing variability of purely mobile network setups in the context of urban temperature sensing~\cite{yang2019designing}. The combination of reference grade stationary sensors and moving sensors also shows promise in other related application, e.g. in the context of pollution island detection~\cite{iyer2022modeling}.\\
In the context of this work, we discuss both ML applications and set a focus on the ability to create virtual sensors from moving sensors in order to increase spatial coverage.
In the following, we introduce several ML algorithms that can be used for regression tasks and discuss how they need to be adapted in order to solve interpolation tasks as well as the advantages and disadvantages of each model. Afterwards, we will decide on one model that will be implemented and evaluated in Chapter~\ref{chap:Evaluation}.

\section{Model Selection Criteria}
\label{sec: model selection criteria}

Before using the ML regression algorithms introduced in this section to solve the interpolation problem, the models need to be adapted to this specific use-case. This can happen either by adapting the input data and the types of features used or by adapting the model configuration. The following questions need to be answered:

\begin{itemize}
    \item \textbf{How to model sensors?:} Do we want to model each sensor individually, do we want to model all sensors in the area, e.g. like a network, or do we want to model the whole area at once similar to image processing?
    \item \textbf{How to model stationary vs. non-stationary sensors?:} Do we model stationary and non-stationary sensors differently and can the model handle the possibly unsteadily data of moving sensors, e.g. different locations or different time intervals?
    \item \textbf{How to model the temporal correlation?:} Does the model allow to model temporal correlation between sensor readings and is it only short-term or also long-term correlation?
    \item \textbf{How to model the spatial correlation?:} Is spatial correlation directly incorporated in the model architecture or does it need to be modelled via features?
\end{itemize}

Next to the adaptations that need to be made in order to fit a regression algorithms to the interpolation problem, there are also non-functional requirements that need to be considered when selecting a model. The most important requirements are:

\begin{itemize}
    \item \textbf{Model Assumptions:} The model assumptions need to be met by the features used in the input data. For example, linear regression assumes that the input features are independent from each other, as linear regression measures the amount the target veariable is influenced by one feature changing while all other features stay the same. In case of correlation, this assumption is violated, as f.e. the amount of percipitation influences the humidity.
    \item \textbf{Accuracy and Reliability:} Creating an accurate and reliable model is really important to increase the trust for predicted values, however there are certain trade-offs to be made, as model performance or generalisation ability are also important factors to consider.
    The accuracy of the model is mainly determined how well the choosen model can fit the underlying data, e.g. a linear model cannot fit a non-linear function, and is measured by the evaluation metrics described in section \ref{subsec:evaluation_metrics}.
    The reliability of the model is determined by the training data as well as the model architecture, as f.e. training data that is not representative of the underlying function can introduce bias into the model or can prevent the model from learning the correct function. Another important factor is the data quality, as more noise can result in worse model performance. Lastly, reliability of the model is also determined by the ability of the model to handle missing or sparse data as well as outliers. This is especially important in our context as we try to integrate moving sensors into the interpolation process, which sense data at different times and locations.
    \item \textbf{Extrapolation Capabilities:} One important factor to consider, especially when using a model with previously unseen data, is the ability to extrapolate data. Linear regression models f.e. try to fit a linear function that can be easily be used with data that is bigger/smaller than previously seen training data as the function is continuous. In comparison, regression trees by default do not allow for extrapolation. Also, Neural Networks (NNs) typically perform unpredictably on unseen data. 
    \item \textbf{Amount of Training Data:} The amount of training data required to train the model is another important factor to consider, as some models require more training data than others. Especially neural networks tend to need more training data than other models, as they have lots of parameters that need to be tuned. In our context, the amount of data available is quite limited, therefore models that require less training data are preferred.
    \item \textbf{Handling of Missing Data:} If the model cannot handle missing data well, there might be additonal data preprocessing steps that need to be done. One example for this would be how the model reacts to not a number (NaN) values which is a float number defined in the IEEE 754 floating-point standard~\cite{ieee754}. Each multiplication with NaN results in NaN, which can therefore lead to a model where all weights turn into NaN when there is a single NaN value in the input data. This is especially important in the context of distributed sensors, as they might not sense every feature at all times. Common strategies to handle missing values involve dropping the complete feature if it has any NaN, drop any rows in the data that has NaN values or imputation, e.g. replace the missing value with a value such as the mean or median of the feature.
    \item \textbf{Handling of Sparse Data:} Similar to missing data, handling of sparse data is also really important to prevent problems such as the NaN problem mentioned previously. The main difference to missing data is that sparse data is not missing, but rather not available at all times. In our context, moving sensors would be an example for sparse data, as they only sense data at certain times and locations. The strategies to handle missing data are also similar to those of missing data, but imputation and interpolation are more common strategies to handle sparse data.
    \item \textbf{Model Performance:} The more complex a model is, generally speaking the more training data it needs to fine-tune all it's weights and the longer it takes to train, either due to the amount of data or the amount of steps that need to be taken when updating weights in the training process. In the context of open-source and citizen participation less complex models are preferred, as they can be trained and deployed with less resources. However, a less complex model could have the downside of not being able to fit the underlying data as well as a more complex model, therefore there is a trade-off between model complexity and model performance.
    \item \textbf{Model Capabilities vs. Interpretability:} A common trade-off in ML models is between model capabilities and complexity and interpretability of the model as done by~\cite{zumwald2021mapping}, as `Neural network and deep learning approaches allow for large flexibility and predictive power but are harder to interpret than ensemble-based approaches which allow for the required flexibility, while still providing insights into the algorithms' inner workings, e.g. via variable importance and prediction uncertainty estimation'.
    \item \textbf{Other:} Next to these main requirements, there are also other requirements, such as the ability the handle massive amounts of data, live retraining or sophisticated support via ML libraries such as \textit{Tensor Flow}~\footnote{\url{https://www.tensorflow.org/}} for commercial use-cases. Due to the limited scope of this work, these requirements will be considered in less detail.
\end{itemize}

After introducing the requirements for model adaptions and selection, the next step is to introduce and compare the different ML regression algorithms.
Generally speaking, ML algorithms can be categorised based on many different properties~\cite{sarker2021machine}, such as the type of learning, e.g. supervised, unsupervised, semi-supervised or reinforment, or the type of problem they try to solve, e.g. classification, regression or clustering. The most important differenciation for this work is to distinguish algorithms based on the type of problem they try to solve. Because we focus on solving interpolation problems, in this work we will only consider algorithms that can be used to solve regression problems, as interpolation is a form of regression.\\
In regression analysis, the goal is to predict a (continous) target variable $y$ based on a set of input variables $X$, like it is the case for temperature interpolation. Generally speaking, this problem can be classified as a supervised learning problem, therefore the possible algorithm candiates are as follows:

\begin{itemize}
    \item (Multi) Linear Regression (MLR)
    \item K-Nearest Neighbours (KNN) Regression
    \item Regression Trees and Forests
    \item Support Vector Regression
    \item (Deep) Neural Networks

    \begin{itemize}
        \item Long-Short Term Memory (LSTM)
        \item RNN
    \end{itemize}
\end{itemize}

% Additonally, there are also other regression such as Regression Trees and Random Forests, Tree Boosting, or Support Vector Regression, which are less popular and seem to be less suitable for the interpolation problem at hand.
Each model has certain benefits but also comes with drawbacks or special assumptions for the input data to the model. First, we will discuss each model and then compare these assumptions with the data coming from the data-layer, to identify suitable models for the task of air temperature interpolation. These models will then later be implemented and compared in chapter \ref{chap:Evaluation}.\\
Based on the domain, there already exist proposed best-practices for which algorithms to use for what applications. In the context of smart cities such recommendations for topics such as intelligent transportation systems, smart grids, smart city health care and more can be found in \cite{ullah2020applications}, which unfortunately does not cover interpolation.

% TODO: table with model comparison for quick overview

\section{Comparison of Machine Learning Algorithms}
\label{sec:comparison ml algorithms}

\subsection{(Multi) Linear Regression}
\label{subsec: linear regression}

Linear regression~\cite{montgomery2021introduction} is a comparatively simple, yet very powerful and widely used model for regression problems. The goal of this model is to predict a continous dependent variable based on a number of independent variables. These independent variables can be either continous or discrete. The model can be expressed as follows:

% TODO: explain equation
\begin{equation}
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon
\end{equation}

where $y$ is the dependent variable, $x_1$ to $x_n$ are the independent variables, $\beta_0$ to $\beta_n$ are the parameters of the model and $\epsilon$ is the error term. The relationship between the parameters is assumed to be liniear, while each variable must be independent of each other. Due to this independent assumption, there are special steps needed to make linear regression work for (geo-) spatial data, as these types of variables are usually correlated with each other. This is further discussed in~\ref{subsec:Dealing with Correlation}.\\
Linear regression has the advantage, that it is a very simple model, therefore the majority of work needs to be done in the feature engineering process. The downside is that there is no inherent support for spatial or temporal correlation and the model cannot be used to fit non-linear functions. In order to fit no-linear functions, polynomial regression can be used, which can however lead to overfitting especially when the degree of the polynomial is high.\\
% TODO
Linear models seem to perform worse in urban temperature related settings, as they are unable to capture non-linear effects~\cite{voelkel2017towards}.

\subsection{KNN Regression}

K-Nearest Neighbours (KNN) is a simple algorithm that can be used for both classification~\cite{cover1967nearest} and regression problems~\cite{altman1992introduction}. The main idea behind KNN is the assumption, that data points near each other are more similar than data points that are further away. As a result, the $k$ nearest neighbours, either by number or by radius, of a data point are used to predict the target variable. The number of nearest neighbours is a hyperparameter that needs to be tuned in order to find the best trade of between bias and variance. The model can be expressed as follows:

% Todo: check if correct
\begin{equation}
    \hat{y} = \frac{1}{k} \sum_{i=1}^{k} y_i
\end{equation}

where $\hat{y}$ is the predicted target variable, $k$ is the number of nearest neighbours and $y_i$ is the target variable of the $i$-th nearest neighbour.\\
KNN is part of the family of non-parametric models, meaining they do not make any strong assumptions about the underlying regression curve. KNN is a simple yet powerful model, however the standard model does not have weights, e.g. all nearest neighbours have the same influence on the prediction, as well as each feature is weighted equally. In the context of correlated input features, this could introduce a bias into the model, if not handled properly in the feature engineering process.

% Todo: more text

\subsection{Regression Trees and Random Forests}

Tree predictors are used for a wide variety of classification and regression problems. Due to the fact that tree predictors are unstable, e.g.\ vary significantly given similar inputs, and tend to overfit, random forests were introduced as a counter measure. Random forests combine multiple tree predictors and train them on different features and subsets of data and either average their predictions in order to reduce the variance or use boosting methods to reduce the bias of a combined estimator~\cite{breiman2001random}.\\
In the case of predicting a continuous target variable, regression trees can be used. The principle behind regression trees is to split the data into continously smaller sub-sets, and organise the splitting points in a way that minimises the error. Compared to decision trees which try to minimise the entropy, regression trees try to minimise an error that is compatible with a continous target variable such as mean squared error (MSE). The model can be expressed as follows:

% TODO: explain equation or remove.
\begin{equation}
    \hat{y} = \sum_{m=1}^M c_m \mathbb{1}(x \in R_m)
\end{equation}

Regression trees are comparitively easy to understand and interpret and offer certain benefits such as feature insensitivity, meaning that features do not need to be scaled before usage and can be used as is. In \cite{zumwald2021mapping} Zumwald et al.\ choose to use Quantile Regression Forests (QRF)~\cite{meinshausen2006quantile} for mapping hyperlocal air temperature in Zurich, Switzerland, due to the flexibility and predictive power of ensemble-based approaches and the ability to still gain additional insights into the algorithms' inner workings via variable importance and prediction uncertainty estimation. They note however, that Neural Network and Deep Learning approaches allow for even larger flexibility and predictive power, but lack behind in other areas such as the aforementioned interpretability.

% TODO: Downsides such as hyperparameter tuning, overfitting, extrapolation, etc.

% Are more ensemble related.
\subsubsection{Bagging Methods}

In order to decrease the variance of a single tree predictor, bagging is used to introduce randomization into the training process. There are several different bagging methods:

\begin{itemize}
    \item \textbf{Pasting}: Splitting the data into different subsets and training a tree predictor on each subset~\cite{breiman1999pasting}
    \item \textbf{Bagging}: Splitting the data into different subsets but with replacement~\cite{breiman1996bagging}
    \item \textbf{Random Subspaces}: Splitting the data into different subsets of features~\cite{ho1998random}
    \item \textbf{Random Patches}: Splitting both samples and features into different subsets~\cite{louppe2012ensembles}
\end{itemize}



% Related work

There are a couple of studies that use regression trees for temperature interpolation. In \cite{venter2020hyperlocal} the authors achive an average RMSE of 0.52 °C (R2 = 0.5), 1.85 °C (R2 = 0.05) and 1.46 °C (R2 = 0.33) for annual mean, daily maximum and minimum air temperature respectively for the city of Oslo, Norway by combining 20 features from satellite data and PWS from the Netatmo network. However, they do not discuss the downsides of regression trees to extrapolate data.

In \cite{zumwald2021mapping} the authors used the quantile regression forest algorithm
use regression trees to predict the daily maximum and minimum air temperature for the city of Zurich, Switzerland. They achieve an average RMSE of 1.5 °C (R2 = 0.7) and 1.4 °C (R2 = 0.7) for the daily maximum and minimum air temperature respectively by combining 20 features from satellite data and PWS from the Netatmo network.\\

In \cite{ho2014mapping}... % TODO: add more text
- good for reference stations
- worse for WOW stations + stations close to the ocean\\

These studies show that regression trees can be used to predict air temperature with a high accuracy on a daily and annual basis for minimum and maximum air temperature with a high spatial resolution, but do not explore the ability to predict actual air temperature for low time resolutions, e.g.\ hourly or sub-hourly.\\
Due to the downside of the inability to extrapolate data which is especially critical in the detection of CUHIs, e.g. air temperature maximas, we do not use regression trees in this work. However, other findings such as station density and features used can be used in this work.\\
\\

% TODO: talk about the inability to capture time series correlation
% TODO: nice to have: add figure of inability to extrapolate maximum temperature
% TODO: QRF method (to create)
There is a systematic bias to underestimate high temperatures and overestimate low temperatures~\cite{zumwald2021mapping, zhang2012bias}

\subsection{Support Vector Machine Regression}

Support Vector Machines (SVM) are typically used in classification problems, however they can be also used for regression problems, called Support Vector Regression (SVR). SVMs transform the input data into a higher dimensional space and try to find a hyperplane that separates the data into two classes. This approach is similar to linear regression, however SVMs are more robust to outliers and can be used for non-linear problems. Depending on the Kernel function used, e.g. Linear, Polynomial, Radial Basis Function (RBF) or Sigmoid, the model can suffer from the same problems as linear regression when dealing with correlated spatial data.
As a result, appropriate counter measures need to be taken, such as using the Mahalanobis distance instead of the Euclidean distance for RBF kernels~\cite{kamada2006support}, which converts correlated features into uncorrelated features.

\subsection{Neural Networks}

- trade off between interpretability and model capabilities
- more data needed to train the network, because there are more weights to train

Neural networks 

\subsubsection{Deep Neural Networks}
 
- approximation theorem
- special form of neural networks with hidden layers

- overparametrization

% TODO

\section{Feature Engineering}

Next to the model, the most important thing for a machine learning algorithm is the data. Even when the model is perfectly suited for the task at hand, if the data is not suitable, e.g. not enough data, wrong quality, wrongly prepared or formatted etc., the model will not be able to perform well. In the following, we take a look at the data coming from the data-layer and discuss important assumptions such as spatial and temporal autocorrelation. These correlations are important to consider, as they could invalidate models as f.e. Linear Regression~\ref{subsec: linear regression} assumes uncorrelated input variables.\\
First of all, the data-layer exposes a variety of single data-points for various features for different locations for current and past points in time.
.Features other than air temperature could further improve the predecition quality, like how~\cite{alonso2020new} suggests that in their study the Normalized Difference Vegetation Index (NDVI) and Modified Normalized Difference Water Index (MNDWI) have a strong impact on their estimation model. Therefore, we discuss how additional features can be included in the model.\\
This model should then be deloyed inside the \textit{service-layer} and act as a building block for further temperature related research and analysis, as air temperature is an important variable for research in agronomy, meteorology, hydrology, ecology and many other fields of application and could be used for UHI detection in the context of smart cities. The general idea and architecture behind the model should not only be applicable for air temperature, but also other types of output features, even tho potentially significant domain knowledge, like in geostatistical analysis and statistics, is required to select and prepare the input features.\\
% TODO: summary of the assumptions for the geolocation data
Generally, there are different types of input features. Firstly, there are discrete measurements that capture the underlying continuous geological process as a specific value at a certain point in time. Examples for this are the measurements from sensors which might be deployed in an urban environment to capture air temperature, humidity and more. Important to note here is the interval, the values are captured. For example, a sensor might capture the air temperature as the average over five minutes, whereas a rain sensor might capture the absolute amount of rain in a specific time interval. Secondly, there are calculated features, which might be derived from the discrete measurements or the location of a measurement. For example, the distance to the closest body of water with a minimum size of $x^2$ meters might be a important information for the air temperature. Lastly, ...\\
% data from gridded data sources (e.g. affiliation to postal code, district...)
In the following chapter, we discuss how different types of correlation between measurements can be taken into account to improve the model and gain additional insights into local (meteological) dynamics.

\subsection{Spatial Autocorrelation}
% How different measurements are spatially related inside a time series

As discussed in chapter \ref{chap:Related Work}, there is a dependency between air temperature and other meteological features and the location of the sensors. The closer a sensor is to another sensor, the more correlated the sensor readings should be.
In geo-statistics, this is called spatial autocorrelation and can be defined traditionally with the Moran's I index~\cite{moran1948interpretation} or the Geary's coefficient~\cite{geary1954contiguity}. 
% TODO: explain autocorrelation and the implications for ML models
% model assumptions: no ML algorithm that expects independence between observations
% feature selection: including all correlated features can lead to overfitting
% spatial feature engineering: add additional features that encode the spatial relationship between sensors
% spatial smoothing:
% spatial aware algorithms: geographically weighted regression (GWR) 

This relationship is however greatly influenced by the type of feature and the location of a sensor. As explained in \cite{oke2006guideline}, the sensor placement in the urban environment is a complex task, as the urban environment is highly dynamic and sensors can be influenced by highly local phenomena, such as air temperarture by heat vents or solar radiation by buildings and surface materials. For this model, we assume that the sensors are placed in a way that the sensor readings are representative for the area they are placed in, even tho in practice this assumption might not hold up, especially for sensors placed by non-experts.\\
Each sensor reading from the data-layer has a location associated with it in the form of longitute, latitude and altitude (if available). This information can be used to calculate the distance between sensors and subsequently the correlation between sensor readings. However, how exactly the distance between two locations is calculated is not trivial. As the earth is a sphere, the distance between two points on the surface is not a straight line. Depending on the application, different distance metrics can be used. For a small area, such as the city of Hamburg, the euclidean distance could be sufficient, as the curvature of the earth for a small distance is negligible. However, for larger areas the geodesic (harvesine) distance might be more appropriate. In this work, we are focussing on a single city including it's surrounding area, therefore the euclidean distance should have a sufficient accuracy while also simplifing the calculation. As the city of Hamburg does not have big differences in elevation, the altitude is not considered in the distance calculation.\\
% grid area -> each data point is in a grid cell. Multiple data points can be in the same grid cell. If this is the case, the readings are averaged. Outliers based on the overall mean temperature are removed. -> this is done in the data layer
The location data can be incorporated into the input data in several ways. The most straight forward way would be to just include the longitude and latitude as input features. Depending on the type of model used, these values might need to be normalised. For example, tree-based models do not require normalisation (cite), as they are not sensitive to the scale of the input features. However, neural networks are very sensitive to the scale of the input features and therefore require normalisation (cite). This approach has the downside, that the distance between sensors is not directly encoded in the input data. If this information is important for the model, it could be included by precalculating the distances between all sensors, however this would increase the complexity of the model especially for large amounts of sensors, as this would result in a quadratic number of input features.\\
The next approach would be to convert the 

- area n x m grid of data points, each grid cell with n x n size (depending on the resulution), 4D space with location as x and y coordinates (euclidean distance -> but only for smaller areas such as city of Hamburg and suroundings), time as z coordinate and feature values as w coordinate.\\

\subsection{Temporal Autocorrelation}
% How different measurements are temporally related inside a time series

- trends
-> expect normally distributed temperature curve
-> temporal lag (e.g. temperature of yesterday has influence on temperature of today)

Either explicitly model 

\subsection{Temporal Cross Correlation}
% How different time series are related to each other (seasonaility)

% TODO
\subsection{Dealing with Correlation}
\label{subsec:Dealing with Correlation}

Todo: Techniques to turn correlated variables into uncorrelated ones
- principal component analysis (PCA)
- ...\\

variance inflation factor (VIF)\\
-> over 10 = invalid model\cite{montgomery2021introduction}
-> others more moderate with max 3 \cite{zuur2010protocol}

\subsection{Dealing with Uncertainty}
- The model has a lot of uncertainty
- model uncertainty in input data? (depending on sensor type, sensor age, placement...)

- dealing with bias
- dealing with variance
-> problem of over-fitting

\subsubsection{Neural Network Models}
input layer -> hidden layers -> output

The advantages of Multi-layer Perceptron are:
- Capability to learn non-linear models.
- Capability to learn models in real-time (on-line learning) using partial\_fit.

The disadvantages of Multi-layer Perceptron (MLP) include:
- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.
- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.
- MLP is sensitive to feature scaling.

- Loss functions/optimzers: Stochastic Gradient Descend, Adam, L-BFGS


- random forest regression
...

\subsection{Evaluation Metrics}
\label{subsec:evaluation_metrics}

- Mean absolute error, mean absolute relative, mean quared error, r-squared, root mean squared error (RMSE)

\section{Model Selection}
% Possible model candidates

Question: do we want to model each sensor individually, do we want to model all sensors at once or do we want to model the whole area?
Do we model stationary and non-stationary sensors differently?

Options:
- linear regression
    - needs independent variables -> PCA
- KNN regression
    - needs independent variables -> PCA
- Neural Network
    - try without independent variables
    - LSTM -> temporal correlation
    - CNN -> spatial correlation, temperature map

After listing the selection criteria, the next step is to discuss the possible model candidates and evaluate them based on the selection criteria. The following models are considered:

- linear regression
- knn regression
- neural networks
    - deep learning
        - sequential model
        - recurrent neural network (RNN)
        - long short-term memory (LSTM)
        - convolutional neural network (CNN)


\section{Machine Learning in Geostatistics}

Idea/Hyphothesis: ML outperforms traditional geostatistical models (in certain scenarios) as it is able to capture more complex interdependencies between features and isn't necessarily bound to the (mathematical) assumptions of geostatistical models.

On important point to mention, is that different meteological features have different interpolation techniques, as they beve different (physical) properties. For example, temperature is a scalar value, while wind speed and direction are vector values. Relative humidity on the other hand is a relativ value that is bound between 0 and 1. For percipitation it gets even more complex, as rainfall is highly connected to cloud coverage and movements. In the following, we will take a look at commonly used existing interpolation techniques for different meteological features and discuss data preprocessing steps.

- Dealing with sparse data
- Dealing with extreme weather events (e.g. heat waves, blizzards, etc.)
- how can a ML model be trained if no such data exists for a given area? -> transfer learning

\subsection{Temperature Interpolation}
- difference between air and surface temperature

surface temperature -> solar radiation, surface roughness, emissivity, soil moisture, soil temperature, vegetation cover, snow cover, and surface slope

air temperature -> surface temperature

\subsection{Wind Speed and Direction Interpolation}
prob not easily archivable with ML -> depends on high/low pressure areas

\subsection{Relative Humidity Interpolation}
Data preprocessing: scale to [0, 1]
does not seem to work well with kriging, maybe just nearest neighbor approach/linear, or the input values are just not accurate

\subsection{Percipitation Interpolation}



DeepLearning Lecture
- sequence modeling design criteria
    - handle variable-length sequences
    - track long term dependencies
    - maintain information about order
    - share parameters across the sequence

% TODO: quick implementation summary
The model is implemented following advice from~\cite{rey2023geographic}...

Additonal ideas:
- types of geological features (vectors, rasters) -> distance, containment, intersection, etc. 

\section{Additional Considerations}

\subsubsection{ML Model Deployment}
Deployed as a service. Input ingested -> continous data map as output all 5 min or so (could also be smaller depending on use-case -> trade-off between cost and accurancy)

\subsubsection{ML Model Retraing}
Need to retrain model from time to time, e.g. if the accurracy drops below a certain threshold.
