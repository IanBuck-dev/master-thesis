\chapter{Machine Learning-based Interpolation}
\label{chap:Machine Learning based Interpolation}
% 10 pages?

% Intro with small motivation why ML is used
In recent times, the area of machine learning has seen big advancements in terms of model size and complexity. Especially in the area of generative AI, transformer-based neural networks have revolutionised text and image generation. Models such as OpenAI's \textit{ChatGPT}~\cite{openai2023gpt4} or Google's \textit{LaMDA}~\cite{thoppilan2022lamda} have generated significant hype for the possiblity of use of AI\@. Additionally, statements like the universal approximation theorem, which states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continous function~\cite{hornik1989multilayer}, emphasize the potential power of ML models.
As a result, the question arises what benefits AI can bring to other areas of application, such as interpolation. In this chapter, we will discuss the usage of ML in the context of data enrichment via interpolation, more precisely in the context of smart cities and air temperature interpolation, and outline possible advantages and disadvantages compared to traditional (geo-) statistical methods and answer questions such as how stationary and moving sensors can be combined to accomplish better interpolation results.\\
% ML in the context of the goal of this work
There are several ways that interpolation can help us in the context of smart cities and air temperature interpolation. First of all, interpolation can help us fill in missing data for individual sensors, which can be caused by sensor failures, network failures or simply maintenance. Secondly, research suggests that a combination of stationary and moving sensors can lead to better sensing capabilities of phenomena such as urban heat or pollution islands (cite). In the context of CUHI detection, moving sensors can increase the sensor coverage and could be used to detect realtionships between staionary sensors and moving sensors, allowing for the detection of a CUHI even if there is currently no moving sensor in the area. Lastly, we could even go so far as to replace traditional (geo-) statistical methods with ML methods, as ML models can be more flexible and can be adapted to different use-cases. This is interesting, as meteorological research and analysis activities are usually in need of gridded or continuous data~\cite{sekulic2020spatio}.\\
In the following, we first introduce several ML algorithms that can be used for regression tasks and discuss how they need to be adapted in order to solve interpolation tasks as well as the advantages and disadvantages of each model. Afterwards, we will decide for one model that will be implemented and evaluated in chapter~\ref{chap:Evaluation}.

\section{AI vs. Machine Learning vs. Deep Learning}

Before diving deeper into the applications of ML, we need to clarify what is meant by artificial intelligence (AI), machine learning (ML) and deep learning (DL). AI is a broad term that is used to describe the ability to perform tasks, that are usually associated with human intelligence. ML is a subfield of AI, that focuses on the ability of a system to learn from data without being explicitly programmed to do so. Finally, DL is a subfield of ML, that uses artificial neural networks, which imitate the structure of the human brain, to learn from data and perform various tasks.\\
% TODO: graphic?

\section{Comparison of Machine Learning Algorithms}
\label{sec:taxonomy}

Before using the ML regression algorithms introduced in this section to solve the interpolation problem, the models need to be adapted to this specific use-case. This can happen either by adapting the input data and the types of features used or by adapting the model configuration. The following questions need to be answered:

\begin{itemize}
    \item \textbf{How to model sensors?:} Do we want to model each sensor individually, do we want to model all sensors in the area or do we want to model the whole area at once?
    \item \textbf{How to model stationary vs. non-stationary sensors?:} Do we model stationary and non-stationary sensors differently and can the model handle the possibly unsteadily data of moving sensors, e.g. different locations or different time intervals?
    \item \textbf{How to model the temporal correlation?:} Does the model allow to model temporal correlation between sensor readings and is it only short-term or also long-term correlation?
    \item \textbf{How to model the spatial correlation?:} Is spatial correlation directly incorporated in the model architecture or does it need to be modelled via features?
\end{itemize}

Next to the questions on how the models need to be adapted, we also need to consider certain selection criteria which is discussed in the next section.

\subsection{Model Selection Criteria}

Next to the adaptations that need to be made in order to fit a regression algorithms to the interpolation problem, there are also non-functional requirements that need to be considered when selecting a model. The most important requirements are:

\begin{itemize}
    \item \textbf{Model Assumptions:} The model assumptions need to be met by the features used in the input data. For example, linear regression assumes that the input features are independent from each other, as linear regression measures the amount the target veariable is influenced by one feature changing while all other features stay the same. In case of correlation, this assumption is violated, as f.e. the amount of percipitation influences the humidity.
    \item \textbf{Accuracy and Reliability:} Creating an accurate and reliable model is really important to increase the trust for predicted values, however there are certain trade-offs to be made, as model performance or generalisation ability are also important factors to consider.
    The accuracy of the model is mainly determined how well the choosen model can fit the underlying data, e.g. a linear model cannot fit a non-linear function, and is measured by the evaluation metrics described in section \ref{subsec:evaluation_metrics}.
    The reliability of the model is determined by the training data as well as the model architecture, as f.e. training data that is not representative of the underlying function can introduce bias into the model or can prevent the model from learning the correct function. Another important factor is the data quality, as more noise can result in worse model performance. Lastly, reliability of the model is also determined by the ability of the model to handle missing or sparse data as well as outliers. This is especially important in our context as we try to integrate moving sensors into the interpolation process, which sense data at different times and locations.
    \item \textbf{Amount of Training Data:} The amount of training data required to train the model is another important factor to consider, as some models require more training data than others. Especially neural networks tend to need more training data than other models, as they have a lot of parameters that need to be tuned. In our context, the amount of data available is quite limited, therefore models that require less training data are preferred.
    \item \textbf{Handling of Missing Data:} TODO
    \item \textbf{Handling of Sparse Data:} TODO
    \item \textbf{Model Performance:} TODO
    \item \textbf{Other:} Next to these main requirements, there are also other requirements to the specifc use case, such as the ability to deploy production-ready models in commercial settings. Due to the limited scope of this work, these requirements will not be considered.
\end{itemize}

After introducing the requirements for model adaptions and selection, the next step is to introduce and compare the different ML regression algorithms.
Generally speaking, ML algorithms can be categorised based on many different properties~\cite{sarker2021machine}, such as the type of learning, e.g. supervised, unsupervised, semi-supervised or reinforment, or the type of problem they try to solve, e.g. classification, regression or clustering. The most important differenciation for this work is to distinguish algorithms based on the type of problem they try to solve. Because we focus on solving interpolation problems, in this work we will only consider algorithms that can be used to solve regression problems, as interpolation is a form of regression.\\
In regression analysis, the goal is to predict a (continous) target variable $y$ based on a set of input variables $X$ (cite), like it is the case for temperature interpolation. Generally speaking, this problem can be classified as a supervised learning problem, therefore the possible algorithm candiates are as follows:
% todo: regression as a way of fitting a continous function and finding the underlying distribution

\begin{itemize}
    \item Linear Regression
    \item KNN Regression
    \item Neural Networks

    \begin{itemize}
        \item LSTM
        \item RNN
    \end{itemize}
\end{itemize}

Additonally, there are also other regression such as Regression Trees and Random Forests, Tree Boosting, or Support Vector Regression, which are less popular and seem to be less suitable for the interpolation problem at hand.
Each model has certain benefits but also comes with drawbacks or special assumptions for the input data to the model. First, we will discuss each model and then compare these assumptions with the data coming from the data-layer, to identify suitable models for the task of air temperature interpolation. These models will then later be implemented and compared in chapter \ref{chap:Evaluation}.\\
Based on the domain, there already exist proposed best-practices for which algorithms to use for what applications. In the context of smart cities such recommendations for topics such as intelligent transportation systems, smart grids, smart city health care and more can be found in \cite{ullah2020applications}, which unfortunately does not cover interpolation.

% TODO: table with model comparison for quick overview

\subsection{Linear Regression}
\label{subsec: linear regression}

Linear regression~\cite{montgomery2021introduction} is a comparatively simple, yet very popular and widely used model for regression problems. The goal of this model is to predict a continous dependent variable based on a number of independent variables. These independent variables can be either continous or discrete. The model can be expressed as follows:

% TODO: explain equation
\begin{equation}
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon
\end{equation}

The relationship between the parameters is assumed to be liniear, while each variable must be independent of each other. Due to this independent assumption, there are special steps needed to make linear regression work for (geo-) spatial data, as these types of variables are usually correlated with each other.

\subsection{KNN Regression}

K-Nearest Neighbours (KNN) is a simple algorithm that can be used for both classification~\cite{cover1967nearest} and regression problems. 

\subsection{Regression Trees and Random Forests}

Decision trees are used for a wide variety of classification problems. Due to the fact that decision trees can vary significantly given similar inputs, e.g have a high variance, random forests were introduced as a counter measure. Random forests combine multiple decision trees and average their predictions in order to reduce the variance.\\ 
When instead of a discrete target variable a continous target variable is predicted, regression trees can be used. The principle behind regression trees is to split the data into continously smaller sub-sets, and asking the correct questions at the right time in order to find the best split. Compared to decision trees which try to minimise the entropy, regression trees try to minimise an error that is compatible with a continous target variable such as mean squared error (MSE). The model can be expressed as follows:

% TODO: explain equation
\begin{equation}
    \hat{y} = \sum_{m=1}^M c_m \mathbb{1}(x \in R_m)
\end{equation}

Regression trees are easy to understand and interpret. Additionally, they do not require any scaling of variables, which is a big advantage compared to other models. However, they are prone to overfitting and are not suitable for extrapolation. Furthermore, they are discontinous at the split points, which is a big disadvantage for interpolation of spatial data. Therefore, regression trees don't seem to be suitable for the task of air temperature interpolation.

\subsection{Tree Boosting}

% TODO

\subsection{Support Vector Machines}

Support Vector Machines (SVM) are typically used in classification problems, however they can be also used for regression problems, called Support Vector Regression (SVR). SVMs transform the input data into a higher dimensional space and try to find a hyperplane that separates the data into two classes. This approach is similar to linear regression, however SVMs are more robust to outliers and can be used for non-linear problems. Depending on the Kernel function used, e.g. Linear, Polynomial, Radial Basis Function (RBF) or Sigmoid, the model can suffer from the same problems as linear regression when dealing with correlated spatial data.
As a result, appropriate counter measures need to be taken, such as using the Mahalanobis distance instead of the Euclidean distance for RBF kernels~\cite{kamada2006support}, which converts correlated features into uncorrelated features.

\subsection{Neural Networks}

Neural networks 

\subsubsection{Deep Neural Networks}

% TODO

\section{Feature Engineering}

Next to the model, the most important thing for a machine learning algorithm is the data. Even when the model is perfectly suited for the task at hand, if the data is not suitable, e.g. not enough data, wrong quality, wrongly prepared or formatted etc., the model will not be able to perform well. In the following, we take a look at the data coming from the data-layer and discuss important assumptions such as spatial and temporal autocorrelation. These correlations are important to consider, as they could invalidate models as f.e. Linear Regression~\ref{subsec: linear regression} assumes uncorrelated input variables.\\
First of all, the data-layer exposes a variety of single data-points for various features for different locations for current and past points in time.
.Features other than air temperature could further improve the predecition quality, like how~\cite{alonso2020new} suggests that in their study the Normalized Difference Vegetation Index (NDVI) and Modified Normalized Difference Water Index (MNDWI) have a strong impact on their estimation model. Therefore, we discuss how additional features can be included in the model.\\
This model should then be deloyed inside the \textit{service-layer} and act as a building block for further temperature related research and analysis, as air temperature is an important variable for research in agronomy, meteorology, hydrology, ecology and many other fields of application and could be used for UHI detection in the context of smart cities. The general idea and architecture behind the model should not only be applicable for air temperature, but also other types of output features, even tho potentially significant domain knowledge, like in geostatistical analysis and statistics, is required to select and prepare the input features.\\
% TODO: summary of the assumptions for the geolocation data
Generally, there are different types of input features. Firstly, there are discrete measurements that capture the underlying continuous geological process as a specific value at a certain point in time. Examples for this are the measurements from sensors which might be deployed in an urban environment to capture air temperature, humidity and more. Important to note here is the interval, the values are captured. For example, a sensor might capture the air temperature as the average over five minutes, whereas a rain sensor might capture the absolute amount of rain in a specific time interval. Secondly, there are calculated features, which might be derived from the discrete measurements or the location of a measurement. For example, the distance to the closest body of water with a minimum size of $x^2$ meters might be a important information for the air temperature. Lastly, ...\\
% data from gridded data sources (e.g. affiliation to postal code, district...)
In the following chapter, we discuss how different types of correlation between measurements can be taken into account to improve the model and gain additional insights into local (meteological) dynamics.

\subsection{Spatial Autocorrelation}
% How different measurements are spatially related inside a time series

As discussed in chapter \ref{chap:Related Work}, there is a dependency between air temperature and other meteological features and the location of the sensors. The closer a sensor is to another sensor, the more correlated the sensor readings should be.
In geo-statistics, this is called spatial autocorrelation and can be defined traditionally with the Moran's I index~\cite{moran1948interpretation} or the Geary's coefficient~\cite{geary1954contiguity}. 
% TODO: explain autocorrelation and the implications for ML models
% model assumptions: no ML algorithm that expects independence between observations
% feature selection: including all correlated features can lead to overfitting
% spatial feature engineering: add additional features that encode the spatial relationship between sensors
% spatial smoothing:
% spatial aware algorithms: geographically weighted regression (GWR) 

This relationship is however greatly influenced by the type of feature and the location of a sensor. As explained in \cite{oke2006guideline}, the sensor placement in the urban environment is a complex task, as the urban environment is highly dynamic and sensors can be influenced by highly local phenomena, such as air temperarture by heat vents or solar radiation by buildings and surface materials. For this model, we assume that the sensors are placed in a way that the sensor readings are representative for the area they are placed in, even tho in practice this assumption might not hold up, especially for sensors placed by non-experts.\\
Each sensor reading from the data-layer has a location associated with it in the form of longitute, latitude and altitude (if available). This information can be used to calculate the distance between sensors and subsequently the correlation between sensor readings. However, how exactly the distance between two locations is calculated is not trivial. As the earth is a sphere, the distance between two points on the surface is not a straight line. Depending on the application, different distance metrics can be used. For a small area, such as the city of Hamburg, the euclidean distance could be sufficient, as the curvature of the earth for a small distance is negligible. However, for larger areas the geodesic (harvesine) distance might be more appropriate. In this work, we are focussing on a single city including it's surrounding area, therefore the euclidean distance should have a sufficient accuracy while also simplifing the calculation. As the city of Hamburg does not have big differences in elevation, the altitude is not considered in the distance calculation.\\
% grid area -> each data point is in a grid cell. Multiple data points can be in the same grid cell. If this is the case, the readings are averaged. Outliers based on the overall mean temperature are removed. -> this is done in the data layer
The location data can be incorporated into the input data in several ways. The most straight forward way would be to just include the longitude and latitude as input features. Depending on the type of model used, these values might need to be normalised. For example, tree-based models do not require normalisation (cite), as they are not sensitive to the scale of the input features. However, neural networks are very sensitive to the scale of the input features and therefore require normalisation (cite). This approach has the downside, that the distance between sensors is not directly encoded in the input data. If this information is important for the model, it could be included by precalculating the distances between all sensors, however this would increase the complexity of the model especially for large amounts of sensors, as this would result in a quadratic number of input features.\\
The next approach would be to convert the 

- area n x m grid of data points, each grid cell with n x n size (depending on the resulution), 4D space with location as x and y coordinates (euclidean distance -> but only for smaller areas such as city of Hamburg and suroundings), time as z coordinate and feature values as w coordinate.\\

\subsection{Temporal Autocorrelation}
% How different measurements are temporally related inside a time series


\subsection{Temporal Cross Correlation}
% How different time series are related to each other (seasonaility)

% TODO
\subsection{Dealing with Correlation}

Todo: Techniques to turn correlated variables into uncorrelated ones
- principal component analysis (PCA)
- ...

\subsection{Dealing with Uncertainty}
- The model has a lot of uncertainty
- model uncertainty in input data? (depending on sensor type, sensor age, placement...)

- dealing with bias
- dealing with variance
-> problem of over-fitting

\subsubsection{Neural Network Models}
input layer -> hidden layers -> output

The advantages of Multi-layer Perceptron are:
- Capability to learn non-linear models.
- Capability to learn models in real-time (on-line learning) using partial\_fit.

The disadvantages of Multi-layer Perceptron (MLP) include:
- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.
- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.
- MLP is sensitive to feature scaling.

- Loss functions/optimzers: Stochastic Gradient Descend, Adam, L-BFGS


- random forest regression
...

\subsection{Evaluation Metrics}
\label{subsec:evaluation_metrics}

- Mean absolute error, mean absolute relative, mean quared error, r-squared, root mean squared error (RMSE)

\section{Model Selection}
% Possible model candidates

Question: do we want to model each sensor individually, do we want to model all sensors at once or do we want to model the whole area?
Do we model stationary and non-stationary sensors differently?

Options:
- linear regression
    - needs independent variables -> PCA
- KNN regression
    - needs independent variables -> PCA
- Neural Network
    - try without independent variables
    - LSTM -> temporal correlation
    - CNN -> spatial correlation, temperature map

After listing the selection criteria, the next step is to discuss the possible model candidates and evaluate them based on the selection criteria. The following models are considered:

- linear regression
- knn regression
- neural networks
    - deep learning
        - sequential model
        - recurrent neural network (RNN)
        - long short-term memory (LSTM)
        - convolutional neural network (CNN)


\section{ML Model Deployment}
Deployed as a service. Input ingested -> continous data map as output all 5 min or so (could also be smaller depending on use-case -> trade-off between cost and accurancy)

\subsection{ML Model Retraing}
Need to retrain model from time to time, e.g. if the accurracy drops below a certain threshold.


% Implementation details
- dealing with coordinate systems:
    - projected coordinate system (e.g. UTM) instead of long lat alt values (important for interpolation)

\section{Machine Learning in Geostatistics}

Idea/Hyphothesis: ML outperforms traditional geostatistical models (in certain scenarios) as it is able to capture more complex interdependencies between features and isn't necessarily bound to the (mathematical) assumptions of geostatistical models.

On important point to mention, is that different meteological features have different interpolation techniques, as they beve different (physical) properties. For example, temperature is a scalar value, while wind speed and direction are vector values. Relative humidity on the other hand is a relativ value that is bound between 0 and 1. For percipitation it gets even more complex, as rainfall is highly connected to cloud coverage and movements. In the following, we will take a look at commonly used existing interpolation techniques for different meteological features and discuss data preprocessing steps.

- Dealing with sparse data
- Dealing with extreme weather events (e.g. heat waves, blizzards, etc.)
- how can a ML model be trained if no such data exists for a given area? -> transfer learning

\subsection{Temperature Interpolation}
- difference between air and surface temperature

surface temperature -> solar radiation, surface roughness, emissivity, soil moisture, soil temperature, vegetation cover, snow cover, and surface slope

air temperature -> surface temperature

\subsection{Wind Speed and Direction Interpolation}
prob not easily archivable with ML -> depends on high/low pressure areas

\subsection{Relative Humidity Interpolation}
Data preprocessing: scale to [0, 1]
does not seem to work well with kriging, maybe just nearest neighbor approach/linear, or the input values are just not accurate

\subsection{Percipitation Interpolation}



DeepLearning Lecture
- sequence modeling design criteria
    - handle variable-length sequences
    - track long term dependencies
    - maintain information about order
    - share parameters across the sequence

% TODO: quick implementation summary
The model is implemented following advice from~\cite{rey2023geographic}...

Additonal ideas:
- types of geological features (vectors, rasters) -> distance, containment, intersection, etc. 
