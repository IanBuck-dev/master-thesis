\chapter{Machine Learning-based Interpolation}
\label{chap:Machine Learning based Interpolation}

In recent times, the area of machine learning has seen big advancements in terms of model size and complexity. Especially in the area of generative AI, transformer-based neural networks have revolutionised text and image generation. Models such as OpenAI's \textit{ChatGPT}~\cite{openai2023gpt4} or Google's \textit{LaMDA}~\cite{thoppilan2022lamda} have generated significant hype for the possiblity of use of AI\@. Additionally, statements like the universal approximation theorem, which states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continous function~\cite{hornik1989multilayer}, emphasize the potential power of ML models. As a result, the question arises what benefits AI can bring to other areas of application, such as interpolation.\\
In this chapter, we will discuss the usage of ML in the context of data enrichment via interpolation, more precisely in the context of smart cities and urban air temperature interpolation. The ML models will be compared in Section~\ref{chap:Evaluation} to traditional proven geostatistical model, e.g.\ Kriging, to outline and discover possible advantages and disadvantages. In general, the idea is to trade the explain- and interpretability of purely statistical-based approaches for model capabilities and accuracy, and the ability to capture more complex (non-linear) dependencies.
Due to the great flexibility of ML models, each model can be fit to completely different use-cases, such as interpolation vs.\ extrapolation or areal interpolation vs.\ interpolation of a single location. The following sections introduce different ML models and discussed their applicability to the use-case of urban air temperature interpolation.

\subsubsection{AI vs. Machine Learning vs. Deep Learning}

Before diving deeper into the applications of ML, we need to clarify what is meant by artificial intelligence (AI), machine learning (ML) and deep learning (DL). AI is a broad term that is used to describe the ability to perform tasks, that are usually associated with human intelligence. ML is a subfield of AI, that focuses on the ability of a system to learn from data without being explicitly programmed to do so. Finally, DL is a subfield of ML, that uses artificial neural networks (ANN), or also called Simulated Neural Network (SNN), which imitate the structure of the human brain, to learn from data and perform various tasks.

\section{Machine-Learning Application Areas in Air Temperarture Interpolation}

As meteorological research and analysis activities are usually in need of gridded or continuous data~\cite{sekulic2020spatio}, interpolation is a really important tool to convert single data points at distinct locations from ground-based weather stations into a continous layer. Interpolation can also be applied to individual sensors in order to fill in missing data that are caused by network outages or to increase the temporal resolution to turn hourly into sub-hourly readings. Especially with the capability to increase the temporal resolution, the question arises how this capability could be combined with moving sensors to increase the spatial coverage of a sensor network. In this work, we will discuss two approaches for the use of interpolation in urban air temperatute sensing:

\begin{itemize}
    \item ML-based interpolation for areas as a substitution for geostatistical methods, e.g. Kriging, to turn individual data points into a continuous grid, possibly with the ability to handle stationary and moving sensor data at the same time
    \item ML-based interpolation for a single location to interpolate time-frames with missing data or to simulate a higher temporal resolution that does not only interpolate between individual data points of the same sensor, but also takes into consideration surrounding sensors
\end{itemize}

Research suggests that for fine-granular spatio-temporal urban air temperature maps, a sensor density of at least 1 sensor per km$^{2}$ is needed~\cite{venter2020hyperlocal}, however the denser the sensor network the better the prediction quality, as even inside a single street canyon air temperatures can easily vary by 2 to 3°C~\cite{sugawara2008temperature}.
% TODO: write about other influence sizes, e.g. 10m -> 1km for different features, + the variability of sensors, e.g. inside a street canyon 2-3°c differences.
In order to achive this sensor density as well as to gain insights into previously unobserved areas and to minimise prediction uncertainty for those areas, hybrid approaches combining stationary and moving sensors have shown to work better than purely stationary networks by covering more ground as well as reducing variability of purely mobile network setups in the context of urban temperature sensing~\cite{yang2019designing}. The combination of reference grade stationary sensors and moving sensors also shows promise in other related application, e.g.\ in the context of pollution island detection~\cite{iyer2022modeling}.\\
In the context of this work, we discuss both ML applications and try to show the feasability and potentials of ML-based interpolation in the context of urban air temperature sensing.
In the following, we introduce several ML algorithms that can be used for regression tasks and discuss how they need to be adapted in order to solve interpolation tasks as well as the advantages and disadvantages of each model. The models will be implemented as prototypes and evaluated in Chapter~\ref{chap:Evaluation}.

\section{Model Selection Criteria}
\label{sec: model selection criteria}

Before using the ML regression algorithms introduced in this section to solve the interpolation problem, the models need to be adapted to this specific use-case. This can happen either by adapting the input data and the types of features used or by adapting the model configuration. The following questions need to be answered:

\begin{itemize}
    \item \textbf{How to model sensors?:} Are sensor locations modelled individually, as a network, or as a grid?
    \item \textbf{How to model the temporal correlation?:} Does the model allow to model temporal correlation between sensor readings and is it only short-term or also long-term correlation?
    \item \textbf{How to model the spatial correlation?:} Is spatial correlation directly incorporated in the model architecture or does it need to be modelled via features?
\end{itemize}

Next to the adaptations that need to be made in order to fit regression algorithms to the interpolation problem, there are also non-functional requirements that need to be considered when selecting a model. The most important requirements are:

\begin{itemize}
    \item \textbf{Model Assumptions:} The model assumptions need to be met by the features used in the input data. For example, linear regression assumes that the input features are independent from each other, as linear regression measures the amount the target veariable is influenced by one feature changing while all other features stay the same. In case of correlation, this assumption is violated, as for example the amount of percipitation influences the humidity. Other assumptions could be the distribution of data, the mean of values not changes, etc.
    \item \textbf{Accuracy and Reliability:} Creating an accurate and reliable model is really important to increase the trust for predicted values, however there are certain trade-offs to be made, as model performance or generalisation ability are also important factors to consider.
    The accuracy of the model is mainly determined how well the choosen model can fit the underlying data, e.g.\ a linear model cannot fit a non-linear function, and is measured by the evaluation metrics described in Section~\ref{sec:validation methodology}.
    The reliability of the model is determined by the training data as well as the model architecture, as f.e.\ training data that is not representative of the underlying function can introduce bias into the model or can prevent the model from learning the correct function. Another important factor is the data quality, as more noise can result in worse model performance. Lastly, reliability of the model is also determined by the ability of the model to handle missing or sparse data as well as outliers. This is especially important in our context as we try to integrate moving sensors into the interpolation process, which sense data at different times and locations.
    \item \textbf{Extrapolation Capabilities:} One important factor to consider, especially when using a model with previously unseen data, is the ability to extrapolate data. Linear regression models f.e.\ try to fit a linear function that can be easily used with data that is bigger/smaller than previously seen training data as the function is continuous. In comparison, regression trees by default do not allow for extrapolation. Also, Neural Networks (NNs) typically perform unpredictably on unseen data.
    \item \textbf{Amount of Training Data:} The amount of training data required to train the model is another important factor to consider, as some models require more training data than others. Especially neural networks tend to need more training data than other models, as they have lots of parameters that need to be tuned. In our context, the amount of data available is quite limited, therefore models that require less training data are preferred.
    \item \textbf{Handling of Missing Data:} If the model cannot handle missing data well, there might be additonal data preprocessing steps that need to be done. One example for this would be how the model reacts to not a number (NaN) values which is a float number defined in the IEEE 754 floating-point standard~\cite{ieee754}. Each multiplication with NaN results in NaN, which can therefore lead to a model where all weights turn into NaN when there is a single NaN value in the input data. This is especially important in the context of distributed sensors, as they might not sense every feature at all times. Common strategies to handle missing values involve dropping the complete feature if it has any NaN, drop any rows in the data that has NaN values or imputation, e.g.\ replace the missing value with a value such as the mean or median of the feature.
    \item \textbf{Handling of Sparse Data:} Similar to missing data, handling of sparse data is also really important to prevent problems such as the NaN problem mentioned previously. The main difference to missing data is that sparse data is not missing, but rather not available at all times. In our context, moving sensors would be an example for sparse data, as they only sense data at certain times and locations. The strategies to handle missing data are also similar to those of missing data, but imputation and interpolation are more common strategies to handle sparse data.
    \item \textbf{Model Performance:} The more complex a model is, generally speaking, the more training data it needs to fine-tune all it's weights and the longer it takes to train, either due to the amount of data or the amount of steps that need to be taken when updating weights in the training process. In the context of open-source and citizen participation less complex models are preferred, as they can be trained and deployed with less resources. However, a less complex model could have the downside of not being able to fit the underlying data as well as a more complex model, therefore there is a trade-off between model complexity and model performance. Another factor is the capability of handling many locations and data points, as some models' performance degrades significantly when the amount of data points increases, as well as the ability to handle high-dimensional data.
    \item \textbf{Model Capabilities vs. Interpretability:} A common trade-off in ML models is between model capabilities and complexity and interpretability of the model as done by~\cite{zumwald2021mapping}, as `Neural network and deep learning approaches allow for large flexibility and predictive power but are harder to interpret than ensemble-based approaches which allow for the required flexibility, while still providing insights into the algorithms' inner workings, e.g.\ via variable importance and prediction uncertainty estimation'.
    \item \textbf{Other:} Next to these main requirements, there are also other requirements, such as the ability the handle massive amounts of data, live retraining or sophisticated support via ML libraries such as \textit{Tensor Flow}~\footnote{\url{https://www.tensorflow.org/}} for commercial use-cases. Due to the limited scope of this work, these requirements will be considered in less detail.
\end{itemize}

After introducing the requirements for model adaptions and selection, the next step is to introduce and compare the different ML regression algorithms.
Generally speaking, ML algorithms can be categorised based on many different properties~\cite{sarker2021machine}, such as the type of learning, e.g.\ supervised, unsupervised, semi-supervised or reinforment, or the type of problem they try to solve, e.g.\ classification, regression or clustering. The most important differenciation for this work is to distinguish algorithms based on the type of problem they try to solve. Because we focus on solving interpolation problems, in this work we will only consider algorithms that can be used to solve regression problems, as interpolation is a form of regression.\\
In regression analysis, the goal is to predict a (continous) target variable $y$ based on a set of input variables $X$, like it is the case for temperature interpolation. Generally speaking, this problem can be classified as a supervised learning problem, therefore possible algorithm candiates contain the following models:

\begin{itemize}
    \item (Multi) Linear Regression (MLR)
    \item K-Nearest Neighbours (KNN) Regression
    \item Regression Trees and Forests
    \item Histogram-Based Gradient Boosting
    \item Support Vector Regression
    \item (Deep) Neural Networks

    \begin{itemize}
        \item Long-Short Term Memory (LSTM)
        \item RNN
    \end{itemize}
\end{itemize}

% TODO: explain why not other methods have been used.

Next to these algorithms, there also exist less popular regression algorithms, such as outlined in~\cite{li2014spatial}, however due to the limited scope of this work, we only take a look at ML models listed above, who are implemented in the popular \textit{sklearn} ML library~\cite{scikit-learn}.
Each model has certain benefits but also comes with drawbacks or special assumptions for the input data to the model. First, we will discuss each model and then compare these assumptions with the data coming from the data-layer, to identify suitable models for the task of air temperature interpolation.
Based on the domain, there already exist proposed best-practices for which algorithms to use for what applications. In the context of smart cities such recommendations exist for topics such as intelligent transportation systems, smart grids, smart city health care and more can be found in~\cite{ullah2020applications}, which does not cover interpolation. In the context of air temperature interpolation, regression forests and histogram-based gradient boosting seem to be popular choices and perform better compared to other methods~\cite{apaydin2022evaluation, ho2014mapping}.

\section{Comparison of Machine Learning Algorithms}
\label{sec:comparison ml algorithms}

\subsection{(Multi) Linear Regression}
\label{subsec: linear regression}

Linear regression~\cite{montgomery2021introduction} is a comparatively simple, yet very powerful and widely used model for regression problems. The goal of this model is to predict a continous dependent variable based on a number of independent variables. These independent variables can be either continous or discrete. The model can be expressed as follows:

% TODO: explain equation
\begin{equation}
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon
\end{equation}

where $y$ is the dependent variable, $x_1$ to $x_n$ are the independent variables, $\beta_0$ to $\beta_n$ are the parameters of the model and $\epsilon$ is the error term. The relationship between the parameters is assumed to be liniear, while each variable must be independent of each other. Due to this independent assumption, there are special steps needed to make linear regression work for (geo-) spatial data, as these types of variables are usually correlated with each other. This is further discussed in~\ref{subsec:Dealing with Correlation}.\\
Linear regression has the advantage, that it is a very simple model, therefore the majority of work needs to be done in the feature engineering process. The downside is that there is no inherent support for spatial or temporal correlation and the model cannot be used to fit non-linear functions. In order to fit no-linear functions, polynomial regression can be used, which can however lead to overfitting especially when the degree of the polynomial is high.\\
Linear models seem to perform worse in urban temperature related settings, as they are unable to capture non-linear effects~\cite{voelkel2017towards}, therefore this model with only be evaluated briefly.

\subsection{KNN Regression}

K-Nearest Neighbours (KNN) is a simple algorithm that can be used for both classification~\cite{cover1967nearest} and regression problems~\cite{altman1992introduction}. The main idea behind KNN is the assumption, that data points near each other are more similar than data points that are further away. As a result, the $k$ nearest neighbours, either by number or by radius, of a data point are used to predict the target variable. The number of nearest neighbours is a hyperparameter that needs to be tuned in order to find the best trade of between bias and variance. The model can be expressed as follows:

% Todo: check if correct
\begin{equation}
    \hat{y} = \frac{1}{k} \sum_{i=1}^{k} y_i
\end{equation}

where $\hat{y}$ is the predicted target variable, $k$ is the number of nearest neighbours and $y_i$ is the target variable of the $i$-th nearest neighbour.\\
KNN is part of the family of non-parametric models, meaining they do not make any strong assumptions about the underlying regression curve. KNN is a simple yet powerful model and based on the weight function, all predictions can either be weighted equally, by distance, or by a custom function, that allows potentially more complex weight calculations.

\subsection{Regression Trees and Random Forests}

Tree predictors are used for a wide variety of classification and regression problems. Due to the fact that tree predictors are unstable, e.g.\ vary significantly given similar inputs, and tend to overfit, random forests were introduced as a counter measure. Random forests combine multiple tree predictors and train them on different features and subsets of data and either average their predictions in order to reduce the variance or use boosting methods to reduce the bias of a combined estimator~\cite{breiman2001random}.\\
In the case of predicting a continuous target variable, regression trees can be used. The principle behind regression trees is to split the data into continously smaller sub-sets, and organise the splitting points in a way that minimises the error. Compared to decision trees which try to minimise the entropy, regression trees try to minimise an error that is compatible with a continous target variable such as mean squared error (MSE). The model can be expressed as follows:

% TODO: explain equation or remove.
\begin{equation}
    \hat{y} = \sum_{m=1}^M c_m \mathbb{1}(x \in R_m)
\end{equation}

Regression trees are comparitively easy to understand and interpret and offer certain benefits such as feature insensitivity, meaning that features do not need to be scaled before usage and can be used as is. In~\cite{zumwald2021mapping} Zumwald et al.\ choose to use Quantile Regression Forests (QRF)~\cite{meinshausen2006quantile} for mapping hyperlocal air temperature in Zurich, Switzerland, due to the flexibility and predictive power of ensemble-based approaches and the ability to still gain additional insights into the algorithms' inner workings via variable importance and prediction uncertainty estimation. They note however, that Neural Network and Deep Learning approaches allow for even larger flexibility and predictive power, but lack behind in other areas such as the aforementioned interpretability. In the following we get an overview of RMSE and R2 values achived by related studies.\\
Ho et al.~\cite{ho2014mapping} mapped maximum daily air temperatures for Vancouver, Canada on hot summer days and combined remote sensing TM/ETM data from Landsat with field observations from Environment Canada and Weather Undergorund (WoW). They compared Ordinary least squares regression, SVM regression, and Random Forest Regression and achived the following RMSE:

\begin{itemize}
    \item Random Forest:\ RMSE 2.31°C
    \item SVM:\ RMSE 2.46°C
    \item Ordinary least squares regression:\ RMSE 2.46°C
\end{itemize}

They also added, that stations closer to the ocean had higher estimation errors, possibly due to more variable wind patterns~\cite{runnalls2000dynamics}.\\
Hengl et al.~\cite{hengl2018random}
RFsp~\cite{hengl2018random} Random Forest for spatial prediction
Random Forest for spatial Interpolation (RFSI)~\cite{sekulic2020random}

There are a couple of studies that use regression trees for temperature interpolation. In \cite{venter2020hyperlocal} the authors achive an average RMSE of 0.52 °C (R2 = 0.5), 1.85 °C (R2 = 0.05) and 1.46 °C (R2 = 0.33) for annual mean, daily maximum and minimum air temperature respectively for the city of Oslo, Norway by combining 20 features from satellite data and PWS from the Netatmo network.

In \cite{zumwald2021mapping} the authors used the quantile regression forest algorithm
use regression trees to predict the daily maximum and minimum air temperature for the city of Zurich, Switzerland. They achieve an average RMSE of 1.5 °C (R2 = 0.7) and 1.4 °C (R2 = 0.7) for the daily maximum and minimum air temperature respectively by combining 20 features from satellite data and PWS from the Netatmo network.\\

There is a systematic bias to underestimate high temperatures and overestimate low temperatures~\cite{zumwald2021mapping, zhang2012bias}

\subsection{Histogram-Based Gradient Boosting}

Similar to Random Forests, Histogram-Based Gradient Boosting (HGB) is an ensemble-based estimator that combines multiple estimators, in this case gradient boosting decision trees, and averages the results to get a more robust estimation. Compared to Random Forests, HGB as implemented by sklearn~\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html}, \textit{last accessed: 25.08.2023}} based on LightGBM~\cite{ke2017lightgbm}, should have better performance, especially for bigger datasets and higher dimensional features, has build-in support for missing values which simplifies data pre-processing steps, and finally seems to slightly outperform RF according to the sklearn documentation~\footnote{\url{https://scikit-learn.org/stable/auto\_examples/ensemble/plot\_forest\_hist\_grad\_boosting\_comparison.html}} and other studies~\cite{apaydin2022evaluation}.

\subsubsection{Bagging Methods}

In order to decrease the variance of a single tree predictor, bagging is used to introduce randomization into the training process. There are several different bagging methods:

\begin{itemize}
    \item \textbf{Pasting}: Splitting the data into different subsets and training a tree predictor on each subset~\cite{breiman1999pasting}
    \item \textbf{Bagging}: Splitting the data into different subsets but with replacement~\cite{breiman1996bagging}
    \item \textbf{Random Subspaces}: Splitting the data into different subsets of features~\cite{ho1998random}
    \item \textbf{Random Patches}: Splitting both samples and features into different subsets~\cite{louppe2012ensembles}
\end{itemize}

Due to the popularity of RF as an extension of bagging methods, we do not consider these bagging methods further in this work.

\subsection{Support Vector Machine Regression}

Support Vector Machines (SVM) are typically used in classification problems, however they can be also used for regression problems, called Support Vector Regression (SVR). SVMs transform the input data into a higher dimensional space and try to find a hyperplane that separates the data into two classes. This approach is similar to linear regression, however SVMs are more robust to outliers and can be used for non-linear problems. Depending on the Kernel function used, e.g. Linear, Polynomial, Radial Basis Function (RBF) or Sigmoid, the model can suffer from the same problems as linear regression when dealing with correlated spatial data.
As a result, appropriate counter measures need to be taken, such as using the Mahalanobis distance instead of the Euclidean distance for RBF kernels~\cite{kamada2006support}, which converts correlated features into uncorrelated features.

\subsection{Neural Networks}

ANNs are a more advanced ML method that takes inspiration from the human brain and electrical impulses being transmitted by neurons. An ANN is build up of neurons which are grouped in layers that are connected and have activation functions and weights, that get trained during the learning process. The most simple ANN is the perceptron~\cite{rosenblatt1957perceptron} which models one single neuron, consisting of one or many inputs, a single processor, and a single output. An ANN usually consists out of one input and output layer, and one to many hidden layers. If there are more than one hidden layer, the ANN is usually called Deep Neural Network (DNN) and we speak from Deep Learning~\cite{lecun2015deep}. ANNs have many hyperparameters and weights to train, therefore they need more data to train. Additionally, ANNs act more like a black box, so model analysis such as feature importance is not possible. Generally speaking, using ANNs is a trade-off between model capability, available data to train and test, and the explainability of the model.\\
As previously mentioned, the universal approximation theorem by Hornik et al.\ states how capable ANNs can be~\cite{hornik1989multilayer}. Depending on how the model is setup, e.g.\ as a feed-forward network or a directed acyclic graph (DAG) where layers feed back into each other, the type of loss-function, and the type of learning method, such as Stochastic Gradient Descend, ANNs can be adapted to many problems such as regression and interpolation.\\
Due to the complexity of ANNs and the lacking explainability of the model, we do not focus on the ML method in this work, however want to keep in mind that ANNs could be a powerful tool for tasks such as LST and air temperature estimation and prediction~\cite{yuan2020deep}.
